{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear regression 1D: Training Two Parameter Stochastic Gradient Descent (SGD)</h1>\n",
    "\n",
    "<h2>Objective</h2><ul><li> How to use SGD(Stochastic Gradient Descent) to train the model.</li></ul> \n",
    "\n",
    "Batch Gradient Descent (BGD) VS GSD:\n",
    "The main difference between BGD and SGD is that BGD uses the entire dataset for each gradient update, while SGD uses a single or small batch of data points for more frequent updates.\n",
    "\n",
    "SGD is often considered superior to BGD because it can converge faster by making frequent updates, potentially escaping local minima and better exploring the loss landscape, and it requires less memory, making it more efficient for large datasets.\n",
    "\n",
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<p>Practice training a model by using Stochastic Gradient descent.</p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Makeup_Data\">Make Some Data</a></li>\n",
    "    <li><a href=\"#Model_Cost\">Create the Model and Cost Function (Total Loss)</a></li>\n",
    "    <li><a href=\"#BGD\">Train the Model:Batch Gradient Descent</a></li>\n",
    "    <li><a href=\"#SGD\">Train the Model:Stochastic gradient descent</a></li>\n",
    "    <li><a href=\"#SGD_Loader\">Train the Model:Stochastic gradient descent with Data Loader</a></li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries we are going to use in the lab.\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Make Some Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the actual data and simulated data\n",
    "torch.manual_seed(1) # Set random seed\n",
    "\n",
    "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "f = 1 * X - 1\n",
    "Y = f + 0.1 * torch.randn(X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Model_Cost\">Create the Model and Cost Function (Total Loss)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step01: Define the forward function\n",
    "def forward(x):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step02: Define the MSE Loss function\n",
    "def criterion(yhat, y):\n",
    "    return torch.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"BGD\">Train the Model: Batch Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters w, b for y = wx + b\n",
    "w = torch.tensor(-15.0, requires_grad = True)\n",
    "b = torch.tensor(-10.0, requires_grad = True)\n",
    "\n",
    "# Define learning rate and create an empty list for containing the loss for each iteration.\n",
    "lr = 0.1\n",
    "LOSS_BGD = []\n",
    "\n",
    "# The function for training the model\n",
    "\n",
    "def train_model(iter):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(iter):\n",
    "        \n",
    "        # make a prediction\n",
    "        Yhat = forward(X)\n",
    "        \n",
    "        # calculate the loss \n",
    "        loss = criterion(Yhat, Y)\n",
    "\n",
    "        # Section for plotting\n",
    "        # get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "        # get_surface.plot_ps()\n",
    "            \n",
    "        # store the loss in the list LOSS_BGD\n",
    "        LOSS_BGD.append(loss)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters slope and bias\n",
    "        w.data = w.data - lr * w.grad.data\n",
    "        b.data = b.data - lr * b.grad.data\n",
    "        \n",
    "        # zero the gradients before running the backward pass\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 10 iterations\n",
    "train_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"SGD\">Train the Model: Stochastic Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model\n",
    "\n",
    "LOSS_SGD = []\n",
    "w = torch.tensor(-15.0, requires_grad = True)\n",
    "b = torch.tensor(-10.0, requires_grad = True)\n",
    "\n",
    "def train_model_SGD(iter):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(iter):\n",
    "        \n",
    "        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n",
    "        Yhat = forward(X)\n",
    "\n",
    "        # store the loss \n",
    "        LOSS_SGD.append(criterion(Yhat, Y).tolist())\n",
    "        \n",
    "        for x, y in zip(X, Y):   ## the main different from BGD is here!!!\n",
    "            \n",
    "            # make a pridiction\n",
    "            yhat = forward(x)\n",
    "        \n",
    "            # calculate the loss \n",
    "            loss = criterion(yhat, y)\n",
    "\n",
    "            # Section for plotting\n",
    "            # get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "        \n",
    "            # backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "            loss.backward()\n",
    "        \n",
    "            # update parameters slope and bias\n",
    "            w.data = w.data - lr * w.grad.data\n",
    "            b.data = b.data - lr * b.grad.data\n",
    "\n",
    "            # zero the gradients before running the backward pass\n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "            \n",
    "        #plot surface and data space after each epoch    \n",
    "        # get_surface.plot_ps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"SGD_Loader\">SGD with Dataset DataLoader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "        self.y = 1 * self.x - 1\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self,index):    \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Return the length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and check the length\n",
    "dataset = Data()\n",
    "print(\"The length of dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "trainloader = DataLoader(dataset = dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for training the model\n",
    "\n",
    "w = torch.tensor(-15.0,requires_grad=True)\n",
    "b = torch.tensor(-10.0,requires_grad=True)\n",
    "LOSS_Loader = []\n",
    "\n",
    "def train_model_DataLoader(epochs):\n",
    "    \n",
    "    # Loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n",
    "        Yhat = forward(X)\n",
    "        \n",
    "        # store the loss \n",
    "        LOSS_Loader.append(criterion(Yhat, Y).tolist())\n",
    "        \n",
    "        for x, y in trainloader:\n",
    "            \n",
    "            # make a prediction\n",
    "            yhat = forward(x)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(yhat, y)\n",
    "            \n",
    "            # Section for plotting\n",
    "            # get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
    "            \n",
    "            # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updata parameters slope\n",
    "            w.data = w.data - lr * w.grad.data\n",
    "            b.data = b.data - lr* b.grad.data\n",
    "            \n",
    "            # Clear gradients \n",
    "            w.grad.data.zero_()\n",
    "            b.grad.data.zero_()\n",
    "            \n",
    "        #plot surface and data space after each epoch    \n",
    "        # get_surface.plot_ps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 iterations\n",
    "train_model_DataLoader(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
